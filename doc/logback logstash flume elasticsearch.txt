http://my.oschina.net/u/1983545/blog/694252  使用Logstash搭建基于日志文件的预警机制

mdc
http://www.zhnhao.com/coding/2015/10/16/Spring%E9%85%8D%E7%BD%AE%E6%97%A5%E8%AE%B0%E2%80%94%E2%80%94%E6%97%A5%E5%BF%97%E4%B8%8E%E5%9B%BD%E9%99%85%E5%8C%96/
http://ketao1989.github.io/posts/LogBack-Implemention-And-Slf4j-Mdc.html

日志优化
<appender name="ASYNC500" class="ch.qos.logback.classic.AsyncAppender">
    <queueSize>500</queueSize>
    <discardingThreshold>0</discardingThreshold>
    <appender-ref ref="FILE" />
</appender>
http://blog.takipi.com/how-to-instantly-improve-your-java-logging-with-7-logback-tweaks/
http://highscalability.com/blog/2015/1/7/the-ultimate-guide-5-methods-for-debugging-production-server.html
在分布式系统中，一个请求可能会经过多个不同的子系统，这时最好生成一个UUID附在请求中，每个子系统在打印日志时都将该UUID放在MDC里，便于后续查询相关的日志。

Elasticsearch - 一个基于Lucene的文档存储，主要用于日志索引、存储和分析。
Fluentd - 日志收集和发出
Flume -分布式日志收集和聚合系统
Graylog2 -具有报警选项的可插入日志和事件分析服务器
Heka -流处理系统，可用于日志聚合
Kibana - 可视化日志和时间戳数据
Logstash -管理事件和日志的工具
Octopussy -日志管理解决方案（可视化/报警/报告）
https://testerhome.com/topics/3026

grok
http://chenlinux.com/2014/10/19/grokdebug-commandline/
https://github.com/elastic/logstash/tree/v1.1.9/patterns
https://github.com/elastic/logstash/blob/v1.1.9/patterns/grok-patterns
http://grokdebug.herokuapp.com/

log collect
http://jasonwilder.com/blog/2012/01/03/centralized-logging/
http://jasonwilder.com/blog/2013/07/16/centralized-logging-architecture/
Log4j本来一统江湖好好的，后来被人说方法上太多同步修饰符，在高并发下性能太烂


地理空间距离
http://tech.meituan.com/lucene-distance.html
http://tech.meituan.com/solr-spatial-search.html

logstash
bin/logstash -f logstash.conf --configtest --verbose
http://bigbo.github.io/pages/2015/08/07/logstash_kafka_new/  logstash的kafka插件使用
http://blog.mmlac.com/how-to-pre-process-logs-with-logstash/
http://blog.zhenchuan.me/elk-logstach-usage/ 
Logstash 使用一个名叫 FileWatch 的 Ruby Gem 库来监听文件变化。这个库支持 glob 展开文件路径，而且会记录一个叫 .sincedb 的数据库文件来跟踪被监听的日志文件的当前读取位置。所以，不要担心 logstash 会漏过你的数据。+
sincedb 文件中记录了每个被监听的文件的 inode, major number, minor number 和 pos。
通常你要导入原有数据进 Elasticsearch 的话，你还需要 filter/date 插件来修改默认的"@timestamp" 字段值。
FileWatch 只支持文件的绝对路径，而且会不自动递归目录。所以有需要的话，请用数组方式都写明具体哪些文件
FileWatch 模块提供了一个稍微简单一点的写法：/path/to/**/*.log，用 ** 来缩写表示递归全部子目录。
start_position 仅在该文件从未被监听过的时候起作用。如果 sincedb 文件中已经有这个文件的 inode 记录了，那么 logstash 依然会从记录过的 pos 开始读取数据。  http://kibana.logstash.es/content/logstash/plugins/input/file.html
filters/date 插件可以用来转换你的日志记录中的时间字符串，变成 LogStash::Timestamp 对象，然后转存到 @timestamp 字段里  Elasticsearch 内部，对时间类型字段，是统一采用 UTC 时间，存成 long 长整形数据的！对日志统一采用 UTC 时间存储
logstash 从 1.5 版本开始才集成了 Kafka 支持。1.5 版本开始所有插件的目录和命名都发生了改变，插件发布地址见：https://github.com/logstash-plugins。 


http://bigbo.github.io/pages/2015/05/23/mozilla_heka/  对日志处理流程如下:
[input] -> [splitter] -> [decoder] -> [filter] -> [encoder] -> [output]
[input]: 输入插件从外部获得数据,并将其读入Heka管道进行处理,支持数据读取方式包含:网络传输(tcp/udp),消息队列支持(AMQP/KAFKA),文件监听等多种方式.
[splitter]: 对数据流进行分流,以区分不同来源日志.
[decoder]: 对数据流进行编码,把 message 的结构各个属性解析出来,支持 Lua 对数据进行编码处理.
[filter]: Heka的处理引擎,可以对数据流进行过滤逻辑,包含统计/聚合甚至做些统计等操作.
[encoder] 相对[decoder]来说是互逆操作,重新把消息内容编码为一定的格式,格式化输出.
[output] 根据指定的匹配规则把某类型的消息输出到外部系统,支持多种协议和方式.
 

filebeat
http://udn.yyuap.com/doc/logstash-best-practice-cn/ecosystem/logstash_forwarder.html   Logstash Forwarder
http://www.tuicool.com/articles/VFrE7nY
https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-configuration-details.html
https://www.elastic.co/guide/en/beats/libbeat/1.0.1/getting-started.html#logstash-setup


http://kibana.logstash.es/content/logstash/plugins/input/collectd.html
http://yhz.me/blog/CentOS-Collectd.html
https://www.dnspod.cn/docs/custom-monitoring.html

elasticsearch
http://www.cnblogs.com/huangfox/p/3543134.html
http://riching.iteye.com/blog/1921625
http://www.cnblogs.com/huangfox/p/3542858.html
http://www.searchly.com/documentation/developer-api-guide/java-jest/
http://www.656463.com/article/vU7jYr.htm

./plugin -install mobz/elasticsearch-head
http://xxx:9200/_plugin/head/
./plugin -install lukas-vlcek/bigdesk
http://xxx:9200/_plugin/bigdesk/#cluster

cluster.name：集群名称，es可以自我发现，拥有相同集群名字的es会构成集群。
node.name：节点名称，当前节点的名字。唯一。
node.master：是否允许当前节点成为master。
node.data：是否允许当前节点存储数据。
index.number_of_shards：一个索引默认的shard数量。
index.number_of_replicas：一个索引默认的副本数量。
path.data：数据存储.
path.log：日志存储。
bootstrap.mlockall：是否只使用内存（不使用swap）。
network.bind_host：设置绑定的ip地址，用于访问es。
network.publish_host：与其他node通信的地址，用于cluster间数据传输。

安装中文分词
index.analysis.analyzer.ik.type : "ik"
将ik config的ik目录copy到es config
copy ik.jar  httpclient common 若干jar到lib

http://www.jianshu.com/p/05cff717563c   Elasticsearch学习笔记
https://github.com/siddontang/elasticsearch-note
http://www.jianshu.com/p/cc6746ac4fc2   url清单
http://segmentfault.com/a/1190000002803609 安全
http://segmentfault.com/a/1190000000369962  js api
http://www.ttlsa.com/nginx/nginx-elasticsearch/   nginx
https://github.com/DmitryKey/luke    gui
http://dockone.io/article/505   我们为Elasticsearch安装了Hdfs Snapshot插件，可以定期将index 备份到Hdfs
https://github.com/garyelephant/blog/blob/master/elasticsearch_optimization_checklist.md   优化
https://www.elastic.co/blog/playing-http-tricks-nginx


http://www.infoq.com/cn/presentations/real-time-analysis-of-massive-operation-data  海量运维数据实时分析  elk


wget https://download.elasticsearch.org/logstash/logstash/logstash-1.3.3-flatjar.jar
wget 'http://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.4.3.tar.gz'
curl -X GET http://localhost:9200/  测试elasticsearch
直接到 elasticsearch data文件夹里删掉就行
 curl -X DELETE 'http://172.16.1.16:9200/logstash-2013.03.*' 
http://www.elasticsearch.org/guide/reference/api/delete/

然后运行 java -jar logstash-1.3.3-flatjar.jar agent -f logstash.conf 即可完成收集入库！ 再运行 java -jar logstash-1.3.3-flatjar.jar web 即可在9292端口访问到 Kibana 界面。
http://blog.csdn.net/cnbird2008/article/details/38762795
Open config/kibana.yml in an editor  Set the elasticsearch_url to point at your Elasticsearch instance
http://logstash.net/docs/1.4.2/tutorials/getting-started-with-logstash
http://logstash.net/docs/1.4.2/filters/grok
https://raw.githubusercontent.com/jordansissel/ruby-grok-ui/master/patterns/grok-patterns

http://my.oschina.net/guol/blog/179848
input { redis { host => "114.215.159.195"  data_type =>"list"  port => "6379"    key => "logstash"   type => "redis-input" }}
output { redis { host => "114.215.159.195"   port => "6379" data_type =>"list"  key => "logstash" } }
java -jar logstash-1.2.2-flatjar.jar agent -f logstash.agent.conf
./logstash -e 'input { file { path => "/data/www/webapp.atsmart.io/app/m5/logs/atsmart.log"  codec => multiline {pattern => "^%{TIMESTAMP_ISO8601} " negate => true   what => previous} } } filter{grok{ match => ["message" , "%{TIMESTAMP_ISO8601:datetime} %{INT:timestamp} %{NOTSPACE:thread} %{LOGLEVEL:level} %{JAVACLASS:class} %{ANY:content}" ] } } output { stdout { codec => "rubydebug"}   }'
./logstash -e 'input { file { path => "/data/www/webapp.atsmart.io/app/m5/logs/atsmart.log"  codec => multiline {pattern => "^%{TIMESTAMP_ISO8601} " negate => true   what => previous} } } filter{grok{ match => ["message" , "%{TIMESTAMP_ISO8601:datetime} %{INT:timestamp} %{NOTSPACE:thread} %{LOGLEVEL:level} %{JAVACLASS:class} %{ANY:content}" ] } } output {    redis { host => "114.215.159.195"   data_type =>"list"  key => "logstash" }  }'

([\s\S]*)同时，也可以用 “([\d\D]*)”、“([\w\W]*)” 来表示任意字符
ANY ([\s\S]*)

./logstash -e 'input { redis { host => "114.215.159.195"  data_type =>"list"  port => "6379"    key => "logstash"   type => "redis-input" }} output {   elasticsearch {   host => "127.0.0.1"   port => "9300"    }}'

Fluentd
http://www.oschina.net/question/12_33599
http://ju.outofmemory.cn/entry/102023
http://ju.outofmemory.cn/entry/95480
td-agent是Fluentd的稳定安装包  curl -L http://toolbelt.treasure-data.com/sh/install-redhat.sh | sh
type webhdfs   Fluentd新版本中添加了对HDFS的支持，out_webhdfs插件
http://m.oschina.net/blog/364018
目前最火的三类Agent：logslash，fluentd，flume；分别用jruby，ruby，java实现
http://eventlog.me/fluentd%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%B7%A5%E5%85%B7%E7%9A%84%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8/


flume
http://phoenix.apache.org/flume.html   flume hbase phoenix
http://shiyanjun.cn/archives/915.html  flume
wget http://mirrors.hust.edu.cn/apache/flume/1.5.0/apache-flume-1.5.0-bin.tar.gz
/home/flume/bin/flume-ng agent --conf /home/flume/conf --conf-file /home/flume/conf/netcat.conf --name agent2 -Dflume.monitoring.type=http -Dflume.monitoring.port=34545   -Dflume.root.logger=DEBUG,console
http://shiyanjun.cn/archives/915.html 
https://cwiki.apache.org/confluence/display/FLUME/Getting+Started
http://abloz.com/2013/02/26/flume-channel-source-sink-summary.html
http://blog.csdn.net/yaoyasong/article/details/39400829
http://my.oschina.net/leejun2005/blog/288136
http://my.oschina.net/88sys/blog/71529
http://zzq635.blog.163.com/blog/static/19526448620139232113947/
https://blogs.apache.org/flume/entry/streaming_data_into_apache_hbase
http://blog.csdn.net/morning_pig/article/details/8534079
org.apache.flume.clients.log4jappender.Log4jAppender
host1.sinks.sink1.type = org.apache.flume.sink.hbase.AsyncHBaseSink
host1.sinks.sink1.type = org.apache.flume.sink.hbase.HBaseSink
org.apache.flume.sink.elasticsearch.ElasticSearchSink
source_agent.sources.apache_server.type = exec
source_agent.sources.apache_server.command = tail -F /opt/muse_tomcat/logs/localhost_access_log..txt
agent1.sources.source1.command = tail -n +0 -F /opt/tomcat/logs/catalina.out
To setup a multi-tier flow, you need to have an avro/thrift sink of first hop pointing to avro/thrift source of the next hop.  In order to flow the data across multiple agents or hops, the sink of the previous agent and source of the current hop need to be avro type with the sink pointing to the hostname (or IP address) and port of the source.

http://shiyanjun.cn/archives/915.html
有两种方式，一种是用来复制（Replication），另一种是用来分流（Multiplexing）。Replication方式，可以将最前端的数据源复制多份，分别传递到多个channel中，每个channel接收到的数据都是相同的
Multiplexing方式，selector可以根据header的值来确定数据传递到哪一个channel
<agent>.sources.<Source1>.selector.type = replicating
<agent>.sources.<Source1>.selector.type = multiplexing
<agent>.sources.<Source1>.selector.header = <someHeader>
<agent>.sources.<Source1>.selector.mapping.<Value1> = <Channel1>
<agent>.sources.<Source1>.selector.default = <Channel2>

Load balancing Sink Processor能够实现load balance功能
a1.sinkgroups.g1.processor.type = load_balance
a1.sinkgroups.g1.processor.selector = round_robin
Spooling Directory Source	监控指定目录内数据变更
另外还有Channel Selector、Sink Processor、Event Serializer、Interceptor等组件

http://blog.javachen.com/2014/07/22/flume-ng.html
log4j 有一个 TimeRolling 的插件，可以把 log4j 分割文件到 spool 目录。基本实现了实时的监控。Flume 在传完文件之后，将会修改文件的后缀，变为 .COMPLETED（后缀也可以在配置文件中灵活指定）。
为了保证输送一定成功，在送到目的地之前，会先缓存数据，待数据真正到达目的地后，删除自己缓存的数据。
Flume 使用事务性的方式保证传送Event整个过程的可靠性。Sink 必须在 Event 被存入 Channel 后，或者，已经被传达到下一站agent里，又或者，已经被存入外部数据目的地之后，才能把 Event 从 Channel 中 remove 掉。
http://tech.meituan.com/mt-log-system-arch.html
http://tech.meituan.com/mt-log-system-optimization.html


<bean id="sentinelConfig"
    class="org.springframework.data.redis.connection.RedisSentinelConfiguration">
    <constructor-arg name="master" value="mymaster" />
    <constructor-arg name="sentinelHostAndPorts">
      <set>
        <value>192.168.88.153:26379</value>
        <value>192.168.88.153:26380</value>
        <value>192.168.88.153:26382</value>
      </set>
    </constructor-arg>
  </bean>